{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1773af-9196-4edd-9bde-960149c7eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regex\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import pickle\n",
    "import csv\n",
    "import glob\n",
    "import difflib\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Load/Save uber_df, nampi_dict and corr_dict\n",
    "def uber_df_load():\n",
    "    with open('uber_df.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "#new\n",
    "def uber_df_load_csv():\n",
    "    df = pd.read_csv(uber_df.csv)\n",
    "    return df\n",
    "\n",
    "def nampi_dict_load():\n",
    "    with open('nampi_dict.pickle', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "#new\n",
    "def nampi_dict_load_csv():\n",
    "    df = pd.read_csv(nampi_dict.csv)\n",
    "    return df\n",
    "        \n",
    "def uber_df_save():\n",
    "    with open('uber_df.pickle', 'wb') as f:\n",
    "        pickle.dump(uber_df, f, pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "#new\n",
    "def uber_df_save_csv():\n",
    "    uber_df.to_csv(uber_df.csv, index=False)\n",
    "\n",
    "def nampi_dict_save():\n",
    "    with open('nampi_dict.pickle', 'wb') as f:\n",
    "        pickle.dump(nampi_dict, f, pickle.HIGHEST_PROTOCOL)  \n",
    "\n",
    "#new\n",
    "def nampi_dict_save_csv():\n",
    "    nampi_di\n",
    "\n",
    "# Lade Liste mit korrekten Namen und Parteien\n",
    "with open(\"list_file.txt\") as f:\n",
    "    name_list = f.read().splitlines()\n",
    "\n",
    "# Funktion,die Namen und Parteien der RednerInnen überprüft und falls nötig korrigiert\n",
    "# Input name = Vorname Name (Partei)\n",
    "# Output: korrekter Name und Partei Vorname Name (Partei)\n",
    "# Code: ChatGPT\n",
    "threshold=0.75\n",
    "def check_name_in_list(name):\n",
    "    if name in name_list:\n",
    "        return name\n",
    "    else:\n",
    "        closest_match = difflib.get_close_matches(name, name_list, n=1, cutoff=threshold)\n",
    "        if closest_match:\n",
    "            if threshold < 0.95:\n",
    "                return closest_match[0]\n",
    "            else:\n",
    "                return \"Incorrect\"\n",
    "        else:\n",
    "            return \"Incorrect\"\n",
    "\n",
    "\n",
    "#Variablen zur Nutzung des Gemeinderats-API    \n",
    "endpunkt_suche = 'http://www.gemeinderat-zuerich.ch/api/Mitglieder?'\n",
    "endpunkt_abfrage = 'http://www.gemeinderat-zuerich.ch/api/Mitglieder/details?'\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name):\n",
    "   \n",
    "    # Open the PDF file\n",
    "    fp = open(file_name, 'rb')\n",
    "\n",
    "    # Create a PDF parser object associated with the file object\n",
    "    parser = PDFParser(fp)\n",
    "\n",
    "    # Create a PDF document object that stores the document structure\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Create a PDF resource manager object that stores shared resources\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "\n",
    "    # Set parameters for analysis\n",
    "    laparams = LAParams()\n",
    "\n",
    "    # Create a PDF page aggregator object\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Save the output of the code to a list\n",
    "    output = []\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        layout = device.get_result()\n",
    "        for obj in layout:\n",
    "            if hasattr(obj, \"get_text\"):\n",
    "                for line in obj:\n",
    "                    for char in line:\n",
    "                        if isinstance(char, LTChar):\n",
    "                            output.append((char.get_text(), char.fontname))\n",
    "\n",
    "    # Process the output word by word\n",
    "    bold_word = \"\"\n",
    "    italic_word = \"\"\n",
    "    nested_list = []\n",
    "    previous_font = \"\"\n",
    "\n",
    "    for char, font in output:\n",
    "        if font == \"Arial-BoldItalicMT\":\n",
    "            if previous_font == \"Arial-ItalicMT\":\n",
    "                nested_list.append([bold_word, italic_word])\n",
    "                bold_word = \"\"\n",
    "                italic_word = \"\"\n",
    "            bold_word += char\n",
    "            previous_font = font\n",
    "        elif font == \"Arial-ItalicMT\":\n",
    "            italic_word += char\n",
    "            previous_font = font\n",
    "        else:\n",
    "            if bold_word:\n",
    "                nested_list.append([bold_word, italic_word])\n",
    "                bold_word = \"\"\n",
    "                italic_word = \"\"\n",
    "\n",
    "    # Check if there's any remaining text that needs to be added to the list\n",
    "    if bold_word:\n",
    "        nested_list.append([bold_word, italic_word])\n",
    "\n",
    "    # If the first element of a sublist is empty, but the second element isnt, add that to the second element of the previous sublist\n",
    "    final_list = []\n",
    "    for i in range(len(nested_list)):\n",
    "        stripped_first_elem = nested_list[i][0].strip()\n",
    "        if stripped_first_elem.startswith(\"ST\"):\n",
    "            continue\n",
    "        elif nested_list[i][0].strip() == \"\" and nested_list[i][1].strip() != \"\":\n",
    "            if final_list:\n",
    "                final_list[-1][1] += nested_list[i][1]\n",
    "        elif nested_list[i][0].strip() != \"\" and nested_list[i][0][:2] != \"ST\":\n",
    "\n",
    "            final_list.append(nested_list[i])\n",
    "        \n",
    "    # Delete spaces at the beginning of the second element of every sublist\n",
    "    for i in range(len(final_list)):\n",
    "        final_list[i][1] = final_list[i][1].lstrip()\n",
    "\n",
    "    print(final_list)\n",
    "    return final_list\n",
    "  \n",
    "\n",
    "#Funktion, die aus der vorangehenden Liste mit den Voten ein erstes dataframe erstellt\n",
    "def build_initial_df (votelist):\n",
    "    df = pd.DataFrame(votelist)\n",
    "    return df\n",
    "\n",
    "#Funktion, welches die Textlängen der Voten zum df hinzufügt\n",
    "def add_length (intitial_df):   \n",
    "    initial_df['Länge'] = initial_df[1].apply(lambda x: len (x)) # Len ermitteln und neue Spalte bilden\n",
    "    return initial_df    \n",
    "\n",
    "#Funktion, welche die noch in den Namen enthaltenen Parteien absplittet und in eine eigene Spalte im df schiebt. Bereinigung einzelner Sonderfälle. \n",
    "def split_parties (df_with_length):\n",
    "    parteien = []\n",
    "    for values in df_with_length[0]:\n",
    "        #print('split parties ' +values)\n",
    "        if values == 'David Garcia Nuñez':\n",
    "            values = 'David Garcia Nuñez (AL)'\n",
    "        elif values == 'Walter Anken':\n",
    "            values = 'Walter Anken (SVP)'\n",
    "        elif values == 'Sven Sobernheim':\n",
    "            values = 'Sven Sobernheim (GLP)'\n",
    "        \n",
    "        \n",
    "        \n",
    "        parteien.append(re.search(r'(?<=\\()(.+?)(?=\\))', values).group())\n",
    "    df_with_length['Partei'] = parteien\n",
    "    \n",
    "    for values in df_with_length[0]:\n",
    "        if '(' in values and ')' in values:\n",
    "            partei = (re.search(r'(?<=\\()(.+?)(?=\\))', values))[1]\n",
    "            name_neu = values.replace('('+partei+')', '')\n",
    "            name_neu = name_neu.replace(':','')\n",
    "            name_neu = name_neu.strip()\n",
    "            name_ohne_partei.append(name_neu)\n",
    "        else:\n",
    "            name_neu = values.strip()\n",
    "            name_ohne_partei.append(name_neu)\n",
    "        \n",
    "    df_with_length[0] = name_ohne_partei\n",
    "    df_with_length[0] = df_with_length[0].str.strip()\n",
    "    \n",
    "    return df_with_length\n",
    "    \n",
    "#Funktion, welche die Namen der RednerInnen aus dem df für die API-Abfrage aufbereitet\n",
    "def build_searchlist (df_with_corrnames):\n",
    "    search_list = []\n",
    "    names = df_with_corrnames[0].values.tolist()\n",
    "    names = [x.replace(' ','+') for x in names] # Space durch + ersetzen\n",
    "    parties = df_with_corrnames['Partei'].values.tolist()\n",
    "    search_list.extend([list(a) for a in zip(names, parties)])\n",
    "    return search_list\n",
    "\n",
    "# Suchabfrage nach Name\n",
    "# Das API ignoriert genaue Namen und Parteizugehörigtkeit und kann deshalbe falsche Treffer zurückgeben.\n",
    "def membersearch(name, partei):    \n",
    "    name = name.strip()\n",
    "    if 'M ichael Schmid' in name:\n",
    "        name.replace('M ichael Schmid', 'Michael Schmid')\n",
    "    pre_filtered = []\n",
    "    req = endpunkt_suche+'name='+name+'&parteiId='+partei+'&includeInactive=true'\n",
    "    r_suche = requests.get(req)\n",
    "    r_suche_out = r_suche.json()\n",
    "    for every in r_suche_out:\n",
    "        if every['Partei'] == partei:\n",
    "            pre_filtered.append(every)\n",
    "    \n",
    "    return pre_filtered    \n",
    "\n",
    "# Funktion die Suchresultate validiert  - welcher Name stimmt wirklich? ACHTUNG: braucht searchname Input\n",
    "# Es wird überprüft, ob Name und Partei des API-Resultats tatsächlich dem Input entspricht\n",
    "# Ausgegeben wird die MID des API, welches jedes Ratsmitglied eindeutig identifziert\n",
    "def memberselect (searchresult):\n",
    "    for every_result in searchresult:        \n",
    "        resname = every_result['Vorname']+' '+every_result['Name']        \n",
    "        snamrep = searchname.replace('+',' ')        \n",
    "        if resname == snamrep:        \n",
    "            corrid = every_result['Id']\n",
    "            return corrid\n",
    "\n",
    "# Funktion, die Mitgliederdetails anhand der ermittelten korrekten MID abfragt\n",
    "# Ausgeben wird eine Liste mit Anrede, Vorname, Name, Partei, Geburtstag, MID, sowie (zur Kontrolle) Name und Partei gemäss dataframe mit den Voten\n",
    "def memberget (mid):\n",
    "    type(mid)\n",
    "    url = endpunkt_abfrage+'mid='+mid\n",
    "    r = requests.get(url)\n",
    "    out = r.json()\n",
    "    vorname = out['Vorname']\n",
    "    name = out['Name']\n",
    "    partei = out['Partei']\n",
    "    nampi = vorname+' ' +name+partei\n",
    "    put = [out['Anrede'], out['Vorname'], out['Name'], out['Partei'], out['Geburtstag'], out['Id'], nampi]\n",
    "    \n",
    "    return put\n",
    "\n",
    "# Funktion, welche eine lokale Datenbank mit bereits früher abgefragten Mitgliederdetails abfragt. \n",
    "def query_nampi_dict (name,partei):\n",
    "    nampi = name+partei\n",
    "    output = nampi_dict[nampi]\n",
    "    return output\n",
    "\n",
    "# Funktion, welche einzelne Schritte/Funktionen zur Abfrage von Mitgliederdetails bündelt (check_nampi, quey_nampi, membersearch, memberselect und memberget)\n",
    "# Ausgegeben Ausgeben wird eine Liste mit Anrede, Vorname, Name, Partei, Geburtstag, MID, sowie (zur Kontrolle) Name und Partei gemäss dataframe mit den Voten\n",
    "# Die lokale Datenbank wird bei der Abfrage bevorzugt, da das API sehr träge ist. So wird jedes Mitglied nur ein einziges Mal über das API abgefragt.\n",
    "def membquest(name, partei):\n",
    "    #print(name)\n",
    "    name = name.replace(':', '')\n",
    "    name = name.strip()  \n",
    "    #print(name)\n",
    "    #print(partei)\n",
    "    if partei == 'FPD':\n",
    "        partei = 'FDP'\n",
    "    nampi = name+partei\n",
    "    #print(nampi)\n",
    "    if nampi in nampi_dict:\n",
    "        put = query_nampi_dict (name, partei)\n",
    "        \n",
    "    else:\n",
    "        searchres = membersearch (name, partei)\n",
    "        midn = memberselect (searchres)    \n",
    "        put = memberget (midn)\n",
    "        nampi = name+partei\n",
    "        nampi_dict[nampi]=put\n",
    "    return put\n",
    "\n",
    "#Funktion, welche das bereits vorhandene df mit Voten, RednerInne, Längen, etc. aufräumt\n",
    "def clean_final_df (final_df):\n",
    "    final_df = final_df.iloc[:, [4,0,3,5,1,2,6,7]] # Reihenfolge der columns\n",
    "    final_df.columns = ['Anrede', 'Name', 'Partei', 'Geburtsdatum', 'Votum', 'Länge', 'Id', 'nampi'] # Beschriftung columns\n",
    "    final_df['Geburtsdatum'] =  pd.to_datetime(final_df['Geburtsdatum'], infer_datetime_format=True) # Geburtsdatum richtig setzen\n",
    "    return final_df    \n",
    "\n",
    "###\n",
    "def readprot(prot_filename):\n",
    "    global sitzungs_id\n",
    "    sitzungs_id = ''.join(re.findall(r'[0-9]', prot_filename))\n",
    "    with open(prot_filename, 'r') as jsprot:\n",
    "        prot = json.load(jsprot)\n",
    "        return prot\n",
    "    \n",
    "#Funktion, welche anhand des jeweiligen filenames das Datum der Sitzung ermittelt\n",
    "def create_sitzungsid (prot_filename):\n",
    "    sitzungs_id = ''.join(re.findall(r'[0-9]', prot_filename))\n",
    "    return sitzungs_id\n",
    "\n",
    "#Funktion, welche die Voten jeder einzelnen Sitzung zu Kontrollzwecken in ein File speichert\n",
    "def save_vote_df (df):\n",
    "    with open('vote_dfs/'+sitzungs_id+'.pickle', 'wb') as f:\n",
    "        pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def save_votelist(votelist):\n",
    "    with open('votelists/'+sitzungs_id+'.pickle', 'wb') as f:\n",
    "              pickle.dump(votelist, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "#Erstellt eine Liste der Protokoll-Files, die eingelesen werden sollen            \n",
    "list_of_pdfs = sorted(glob.glob('2018/*.pdf'))\n",
    "\n",
    "#Erstellt eine Liste der Protokoll-Files, die eingelesen werden sollen\n",
    "list_of_json = sorted(glob.glob('betatest/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec06fc73-324c-4a29-a3d2-b34f4f1e91bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betatest/GR-Protokoll 20180523.002 substanziell.json\n",
      "betatest/GR-Protokoll 20180530.003 substanziell.json\n",
      "betatest/GR-Protokoll 20180606.004 substanziell.json\n",
      "betatest/GR-Protokoll 20180613.005 substanziell.json\n",
      "betatest/GR-Protokoll 20180620.006 substanziell.json\n",
      "betatest/GR-Protokoll 20180627.007 substanziell.json\n",
      "betatest/GR-Protokoll 20180704.008 substanziell.json\n",
      "betatest/GR-Protokoll 20180711.009 substanziell.json\n",
      "betatest/GR-Protokoll 20180711.010 substanziell.json\n",
      "betatest/GR-Protokoll 20180822.011 substanziell.json\n",
      "betatest/GR-Protokoll 20180829.012 substanziell.json\n",
      "betatest/GR-Protokoll 20180905.013 substanziell.json\n",
      "betatest/GR-Protokoll 20180912.014 substanziell.json\n",
      "betatest/GR-Protokoll 20180919.015 substanziell.json\n",
      "betatest/GR-Protokoll 20180926.016 substanziell.json\n",
      "betatest/GR-Protokoll 20181003.017 substanziell.json\n",
      "betatest/GR-Protokoll 20181024.018 substanziell.json\n",
      "betatest/GR-Protokoll 20181031.019 substanziell.json\n",
      "betatest/GR-Protokoll 20181107.020 substanziell.json\n",
      "betatest/GR-Protokoll 20181114.021 substanziell.json\n",
      "betatest/GR-Protokoll 20181114.022 substanziell.json\n",
      "betatest/GR-Protokoll 20181121.023 substanziell.json\n",
      "betatest/GR-Protokoll 20181128.024 substanziell.json\n",
      "betatest/GR-Protokoll 20181205.025 substanziell.json\n",
      "betatest/GR-Protokoll 20181212.026 substanziell.json\n",
      "betatest/GR-Protokoll 20181212.027 substanziell.json\n",
      "betatest/GR-Protokoll 20181212.028 substanziell.json\n",
      "betatest/GR-Protokoll 20181214.029 substanziell.json\n",
      "betatest/GR-Protokoll 20181214.030 substanziell.json\n",
      "betatest/GR-Protokoll 20181214.031 substanziell.json\n",
      "betatest/GR-Protokoll 20181219.032 substanziell.json\n",
      "betatest/GR-Protokoll 20190109.033 substanziell.json\n",
      "betatest/GR-Protokoll 20190116 034 substanziell.json\n",
      "betatest/GR-Protokoll 20190123.035 substanziell.json\n",
      "betatest/GR-Protokoll 20190130.036 substanziell.json\n",
      "betatest/GR-Protokoll 20190130.037 substanziell.json\n",
      "betatest/GR-Protokoll 20190206.038 substanziell.json\n",
      "betatest/GR-Protokoll 20190227.039 substanziell.json\n",
      "betatest/GR-Protokoll 20190306.040 substanziell.json\n",
      "betatest/GR-Protokoll 20190313.041 substanziell.json\n",
      "betatest/GR-Protokoll 20190320.042 substanziell.json\n",
      "betatest/GR-Protokoll 20190327.043 substanziell.json\n",
      "betatest/GR-Protokoll 20190403.044 substanziell.json\n",
      "betatest/GR-Protokoll 20190410.045 substanziell.json\n",
      "betatest/GR-Protokoll 20190417.046 substanziell.json\n",
      "betatest/GR-Protokoll 20190515.048 substanziell.json\n",
      "betatest/GR-Protokoll 20190522.049 substanziell.json\n",
      "betatest/GR-Protokoll 20190605.050 substanziell.json\n",
      "betatest/GR-Protokoll 20190612.051 substanziell.json\n",
      "betatest/GR-Protokoll 20190619.052 substanziell.json\n",
      "betatest/GR-Protokoll 20190619.053 substanziell.json\n",
      "betatest/GR-Protokoll 20190626.054 substanziell.json\n",
      "betatest/GR-Protokoll 20190703.055 substanziell.json\n",
      "betatest/GR-Protokoll 20190703.056 substanziell.json\n",
      "betatest/GR-Protokoll 20190710.057 substanziell.json\n",
      "betatest/GR-Protokoll 20190710.058 substanziell.json\n",
      "betatest/GR-Protokoll 20190821.059 substanziell.json\n",
      "betatest/GR-Protokoll 20190828.060 substanziell.json\n",
      "betatest/GR-Protokoll 20190904.061 substanziell.json\n",
      "betatest/GR-Protokoll 20190911.062 substanziell.json\n",
      "betatest/GR-Protokoll 20190918.063 substanziell.json\n",
      "betatest/GR-Protokoll 20190925.064 substanziell.json\n",
      "betatest/GR-Protokoll 20190925.065 substanziell.json\n",
      "betatest/GR-Protokoll 20190925.066 substanziell.json\n",
      "betatest/GR-Protokoll 20191002.067 substanziell.json\n",
      "betatest/GR-Protokoll 20191023.068 substanziell.json\n",
      "betatest/GR-Protokoll 20191030.069 substanziell.json\n",
      "betatest/GR-Protokoll 20191030.070 substanziell.json\n",
      "betatest/GR-Protokoll 20191106.071 substanziell.json\n",
      "betatest/GR-Protokoll 20191113.072 substanziell.json\n",
      "betatest/GR-Protokoll 20191113.073 substanziell.json\n",
      "betatest/GR-Protokoll 20191120.074 substanziell.json\n",
      "betatest/GR-Protokoll 20191127.075 substanziell.json\n",
      "betatest/GR-Protokoll 20191127.076 substanziell.json\n",
      "betatest/GR-Protokoll 20191204.077 substanziell.json\n",
      "betatest/GR-Protokoll 20191211.078 substanziell.json\n",
      "betatest/GR-Protokoll 20191211.079 substanziell.json\n",
      "betatest/GR-Protokoll 20191211.080 substanziell.json\n",
      "betatest/GR-Protokoll 20191213.081 substanziell.json\n",
      "betatest/GR-Protokoll 20191213.082 substanziell.json\n",
      "betatest/GR-Protokoll 20191218.084 substanziell.json\n",
      "betatest/GR-Protokoll 20200108.085 substanziell.json\n",
      "betatest/GR-Protokoll 20200115.086 substanziell.json\n",
      "betatest/GR-Protokoll 20200122.087 substanziell.json\n",
      "betatest/GR-Protokoll 20200129.088 substanziell.json\n",
      "betatest/GR-Protokoll 20200205.090 substanziell.json\n",
      "betatest/GR-Protokoll 20200226.091 substanziell.json\n",
      "betatest/GR-Protokoll 20200304.092 substanziell.json\n",
      "betatest/GR-Protokoll 20200506.094 substanziell.json\n",
      "betatest/GR-Protokoll 20200513.095 substanziell.json\n",
      "betatest/GR-Protokoll 20200527.096 substanziell.json\n",
      "betatest/GR-Protokoll 20200603.097 substanziell.json\n",
      "betatest/GR-Protokoll 20200610.098 substanziell.json\n",
      "betatest/GR-Protokoll 20200617.099 substanziell.json\n",
      "betatest/GR-Protokoll 20200624.100 substanziell.json\n",
      "betatest/GR-Protokoll 20200701.101 substanziell.json\n",
      "betatest/GR-Protokoll 20200708.102 substanziell.json\n",
      "betatest/GR-Protokoll 20200708.103 substanziell.json\n",
      "betatest/GR-Protokoll 20200819.104 substanziell.json\n",
      "betatest/GR-Protokoll 20200826.105 substanziell.json\n",
      "betatest/GR-Protokoll 20200902.106 substanziell.json\n",
      "betatest/GR-Protokoll 20200909.107 substanziell.json\n",
      "betatest/GR-Protokoll 20200923.108 substanziell.json\n",
      "betatest/GR-Protokoll 20200930.109 substanziell.json\n",
      "betatest/GR-Protokoll 20200930.110 substanziell.json\n",
      "betatest/GR-Protokoll 20201021.111 substanziell.json\n",
      "betatest/GR-Protokoll 20201028.112 substanziell.json\n",
      "betatest/GR-Protokoll 20201104.113 substanziell.json\n",
      "betatest/GR-Protokoll 20201111.114 substanziell.json\n",
      "betatest/GR-Protokoll 20201118.115 substanziell.json\n",
      "betatest/GR-Protokoll 20201125.116 substanziell.json\n",
      "betatest/GR-Protokoll 20201202.117 substanziell.json\n",
      "betatest/GR-Protokoll 20201209.118 substanziell.json\n",
      "betatest/GR-Protokoll 20201209.119 substanziell.json\n",
      "betatest/GR-Protokoll 20201209.120 substanziell.json\n",
      "betatest/GR-Protokoll 20201211.121 substanziell.json\n",
      "betatest/GR-Protokoll 20201211.122 substanziell.json\n",
      "betatest/GR-Protokoll 20201211.123 substanziell.json\n",
      "betatest/GR-Protokoll 20201216.124 substanziell.json\n",
      "betatest/GR-Protokoll 20201216.125 substanziell.json\n",
      "betatest/GR-Protokoll 20201216.126 substanziell.json\n",
      "betatest/GR-Protokoll 20210106.127 substanziell.json\n",
      "betatest/GR-Protokoll 20210113.128 substanziell.json\n",
      "betatest/GR-Protokoll 20210120.129 substanziell.json\n",
      "betatest/GR-Protokoll 20210127.130 substanziell.json\n",
      "betatest/GR-Protokoll 20210203.131 substanziell.json\n",
      "betatest/GR-Protokoll 20210210.132 substanziell.json\n",
      "betatest/GR-Protokoll 20210303.133 substanziell.json\n",
      "betatest/GR-Protokoll 20210310.134 substanziell.json\n",
      "betatest/GR-Protokoll 20210317.135 substanziell.json\n",
      "betatest/GR-Protokoll 20210324.136 substanziell.json\n",
      "betatest/GR-Protokoll 20210331.137 substanziell.json\n",
      "betatest/GR-Protokoll 20210407.138 substanziell.json\n",
      "betatest/GR-Protokoll 20210407.139 substanziell.json\n",
      "betatest/GR-Protokoll 20210407.140 substanziell.json\n",
      "betatest/GR-Protokoll 20210409.141 substanziell.json\n",
      "betatest/GR-Protokoll 20210409.142 substanziell.json\n",
      "betatest/GR-Protokoll 20210410.143 substanziell.json\n",
      "betatest/GR-Protokoll 20210410.144 substanziell.json\n",
      "betatest/GR-Protokoll 20210414.145 substanziell.json\n",
      "betatest/GR-Protokoll 20210421.146 substanziell.json\n",
      "betatest/GR-Protokoll 20210519.147 substanziell.json\n",
      "betatest/GR-Protokoll 20210526.148 substanziell.json\n",
      "betatest/GR-Protokoll 20210602.149 substanziell.json\n",
      "betatest/GR-Protokoll 20210609.150 substanziell.json\n",
      "betatest/GR-Protokoll 20210616.151 substanziell.json\n",
      "betatest/GR-Protokoll 20210623.152 substanziell.json\n",
      "betatest/GR-Protokoll 20210630.153 substanziell.json\n",
      "betatest/GR-Protokoll 20210630.154 substanziell.json\n",
      "betatest/GR-Protokoll 20210630.155 substanziell.json\n",
      "betatest/GR-Protokoll 20210702.156 substanziell.json\n",
      "betatest/GR-Protokoll 20210707.157 substanziell.json\n",
      "betatest/GR-Protokoll 20210714.158 substanziell.json\n",
      "betatest/GR-Protokoll 20210714.159 substanziell.json\n",
      "betatest/GR-Protokoll 20210825.160 substanziell.json\n",
      "betatest/GR-Protokoll 20210901.161 substanziell.json\n",
      "betatest/GR-Protokoll 20210908.162 substanziell.json\n",
      "betatest/GR-Protokoll 20210915.163 substanziell.json\n",
      "betatest/GR-Protokoll 20210922.164 substanziell.json\n",
      "betatest/GR-Protokoll 20210929.165 substanziell.json\n",
      "betatest/GR-Protokoll 20211006.166 substanziell.json\n",
      "betatest/GR-Protokoll 20211027.167 substanziell.json\n",
      "betatest/GR-Protokoll 20211103.168 substanziell.json\n",
      "betatest/GR-Protokoll 20211110.169 substanziell.json\n",
      "betatest/GR-Protokoll 20211124.171 substanziell.json\n",
      "betatest/GR-Protokoll 20211201.172 substanziell.json\n",
      "betatest/GR-Protokoll 20211208.173 substanziell.json\n",
      "betatest/GR-Protokoll 20211208.174 substanziell.json\n",
      "betatest/GR-Protokoll 20211208.175 substanziell.json\n",
      "betatest/GR-Protokoll 20211210.176 substanziell.json\n",
      "betatest/GR-Protokoll 20211210.177 substanziell.json\n",
      "betatest/GR-Protokoll 20211210.178 substanziell.json\n",
      "betatest/GR-Protokoll 20211215.179 substanziell.json\n",
      "betatest/GR-Protokoll 20220105.180 substanziell.json\n",
      "betatest/GR-Protokoll 20220112.181 substanziell.json\n",
      "betatest/GR-Protokoll 20220126.183 substanziell.json\n",
      "betatest/GR-Protokoll 20220202.184 substanziell.json\n",
      "betatest/GR-Protokoll 20220209.185 substanziell.json\n",
      "betatest/GR-Protokoll 20220302.186 substanziell.json\n",
      "betatest/GR-Protokoll 20220309.187 substanziell.json\n",
      "betatest/GR-Protokoll 20220316.188 substanziell.json\n",
      "betatest/GR-Protokoll 20220319.189 substanziell.json\n",
      "betatest/GR-Protokoll 20220319.190 substanziell.json\n",
      "betatest/GR-Protokoll 20220323.191 substanziell.json\n",
      "betatest/GR-Protokoll 20220330.192 substanziell.json\n",
      "betatest/GR-Protokoll 20220406.193 substanziell.json\n",
      "betatest/GR-Protokoll 20220413.194 substanziell.json\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize uber_df\n",
    "if exists('uber_df.pickle') == True:\n",
    "    uber_df = uber_df_load()\n",
    "else:\n",
    "    uber_df = pd.DataFrame(columns=['Anrede','Name','Partei','Geburtsdatum','Sitzung','Länge','Id', 'nampi'])\n",
    "\n",
    "# Load or initialize nampi_dict\n",
    "if exists('nampi_dict.pickle') == True:\n",
    "    nampi_dict = nampi_dict_load()\n",
    "else:\n",
    "    nampi_dict = {} # key=nampi value=list'Anrede', 'Vorname', 'Name', 'Partei', 'Geburtstag', 'Id'\n",
    "\n",
    "def part1 (every_pdf):\n",
    "    global sitzungs_id\n",
    "    sitzungs_id = ''.join(re.findall(r'[0-9]', every_pdf))\n",
    "    \n",
    "    final_list=extract_pdf_text(every_pdf)\n",
    "    \n",
    "    for i in range(len(final_list)):\n",
    "        input_list[i][0] = check_name_in_list(input_list[i][0])\n",
    "    \n",
    "    votelist = final_list\n",
    "    save_votelist (votelist)\n",
    "        \n",
    "    global initial_df\n",
    "    initial_df = pd.DataFrame(votelist)\n",
    "    #initial_df = initial_df[initial_df[0].apply(lambda x: not x.startswith('ST'))]\n",
    "    initial_df = initial_df[initial_df[0].apply(lambda x: not x.startswith('Pierre Heusser'))]\n",
    "    \n",
    "    #initial_df[0] = initial_df[0].apply(lambda x: x.replace(' :', ''))\n",
    "    #initial_df[0] = initial_df[0].apply(lambda x: x.replace(':', ''))\n",
    "    #initial_df[0] = initial_df[0].apply(lambda x: x.replace(' nimmt Stellung', ''))\n",
    "        \n",
    "    df_with_length = add_length(initial_df)\n",
    "    #save_vote_df(df_with_length)\n",
    "    \n",
    "    global name_ohne_partei\n",
    "    name_ohne_partei =[]\n",
    "    \n",
    "    global df_in_progress\n",
    "    df_in_progress = split_parties (df_with_length)\n",
    "       \n",
    "    search_list = build_searchlist (df_in_progress)\n",
    "    return search_list\n",
    "        \n",
    "for every_json in list_of_json:\n",
    "    print(every_json)\n",
    "    search_list = part1 (every_json)\n",
    "    resultlist = []\n",
    "    for every_name in search_list:\n",
    "        #print (every_name)\n",
    "        global searchname\n",
    "        searchname = every_name[0]\n",
    "        #print('searchname='+searchname)\n",
    "        searchpartei = every_name[1]\n",
    "        outp = membquest (searchname, searchpartei)\n",
    "        resultlist.append(outp)\n",
    "        \n",
    "    nampi_dict_save()\n",
    "    \n",
    "    result_df = pd.DataFrame(resultlist)    \n",
    "    result_df.drop(columns=[1,2,3], inplace=True)\n",
    "    final_df = pd.concat([df_in_progress, result_df], axis=1)\n",
    "    final_df = clean_final_df(final_df)\n",
    "    move_df = final_df.copy(deep=True)\n",
    "    \n",
    "    # Prep to move to uber_df\n",
    "    move_df.insert(4,'Sitzung','')\n",
    "    move_df['Sitzung'] = sitzungs_id\n",
    "    move_df.drop(['Votum'], axis=1, inplace=True)\n",
    "    \n",
    "    # !!! Index ist noch falsch, nicht fortlaufend\n",
    "    uber_df = uber_df.append(move_df)\n",
    "    #uber_df.reset_index()\n",
    "    uber_df_save()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575448c-88e5-4b7f-b034-3df9829cd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nampi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b32a563-f51e-4684-9ee6-ab034b1f40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_corrnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec90e7-d349-4fba-8db0-194247b2a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_df[uber_df['Name'].str.contains(\"Strub\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c32664-9bf9-47f2-86a5-d876523dc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4199d552-90ae-492f-a69c-286f45a6fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anrede</th>\n",
       "      <th>Name</th>\n",
       "      <th>Partei</th>\n",
       "      <th>Geburtsdatum</th>\n",
       "      <th>Sitzung</th>\n",
       "      <th>Länge</th>\n",
       "      <th>Id</th>\n",
       "      <th>nampi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Herr</td>\n",
       "      <td>David Garcia Nuñez</td>\n",
       "      <td>AL</td>\n",
       "      <td>1975-05-16</td>\n",
       "      <td>20180523002</td>\n",
       "      <td>455</td>\n",
       "      <td>4a1be35e-3f69-48a4-85b1-3fee07b006ae</td>\n",
       "      <td>David Garcia NuñezAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Michael Schmid</td>\n",
       "      <td>FDP</td>\n",
       "      <td>1976-05-21</td>\n",
       "      <td>20180523002</td>\n",
       "      <td>292</td>\n",
       "      <td>f68282d3-d683-48cf-ade1-c08da3dd76da</td>\n",
       "      <td>Michael SchmidFDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frau</td>\n",
       "      <td>Heidi Egger</td>\n",
       "      <td>SP</td>\n",
       "      <td>1962-12-15</td>\n",
       "      <td>20180523002</td>\n",
       "      <td>1432</td>\n",
       "      <td>70c4fb44-48da-47b2-b24e-aaf500c1e78c</td>\n",
       "      <td>Heidi EggerSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Stephan Iten</td>\n",
       "      <td>SVP</td>\n",
       "      <td>1979-01-10</td>\n",
       "      <td>20180523002</td>\n",
       "      <td>1858</td>\n",
       "      <td>ce8041b1-19af-4e73-8c30-0b2577592bbd</td>\n",
       "      <td>Stephan ItenSVP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Thomas Kleger</td>\n",
       "      <td>FDP</td>\n",
       "      <td>1980-09-12</td>\n",
       "      <td>20180523002</td>\n",
       "      <td>286</td>\n",
       "      <td>510f49c8-76b0-4c37-a031-518867986a26</td>\n",
       "      <td>Thomas KlegerFDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Markus Baumann</td>\n",
       "      <td>GLP</td>\n",
       "      <td>1973-09-19</td>\n",
       "      <td>20220413194</td>\n",
       "      <td>3011</td>\n",
       "      <td>32e89ef8-f5b2-46e5-aafe-7dd15929f00a</td>\n",
       "      <td>Markus BaumannGLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Alan David Sangines</td>\n",
       "      <td>SP</td>\n",
       "      <td>1986-05-09</td>\n",
       "      <td>20220413194</td>\n",
       "      <td>4191</td>\n",
       "      <td>3c84712a-df95-48f7-b76e-d95540209bdb</td>\n",
       "      <td>Alan David SanginesSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Herr</td>\n",
       "      <td>David Garcia Nuñez</td>\n",
       "      <td>AL</td>\n",
       "      <td>1975-05-16</td>\n",
       "      <td>20220413194</td>\n",
       "      <td>883</td>\n",
       "      <td>4a1be35e-3f69-48a4-85b1-3fee07b006ae</td>\n",
       "      <td>David Garcia NuñezAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Mischa Schiwow</td>\n",
       "      <td>AL</td>\n",
       "      <td>1961-08-31</td>\n",
       "      <td>20220413194</td>\n",
       "      <td>3602</td>\n",
       "      <td>ddcd41f6-9873-4002-80d8-dfd87114c9a5</td>\n",
       "      <td>Mischa SchiwowAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Herr</td>\n",
       "      <td>Mischa Schiwow</td>\n",
       "      <td>AL</td>\n",
       "      <td>1961-08-31</td>\n",
       "      <td>20220413194</td>\n",
       "      <td>4240</td>\n",
       "      <td>ddcd41f6-9873-4002-80d8-dfd87114c9a5</td>\n",
       "      <td>Mischa SchiwowAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9185 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Anrede                 Name Partei Geburtsdatum      Sitzung Länge  \\\n",
       "0    Herr   David Garcia Nuñez     AL   1975-05-16  20180523002   455   \n",
       "1    Herr       Michael Schmid    FDP   1976-05-21  20180523002   292   \n",
       "2    Frau          Heidi Egger     SP   1962-12-15  20180523002  1432   \n",
       "3    Herr         Stephan Iten    SVP   1979-01-10  20180523002  1858   \n",
       "4    Herr        Thomas Kleger    FDP   1980-09-12  20180523002   286   \n",
       "..    ...                  ...    ...          ...          ...   ...   \n",
       "13   Herr       Markus Baumann    GLP   1973-09-19  20220413194  3011   \n",
       "14   Herr  Alan David Sangines     SP   1986-05-09  20220413194  4191   \n",
       "15   Herr   David Garcia Nuñez     AL   1975-05-16  20220413194   883   \n",
       "16   Herr       Mischa Schiwow     AL   1961-08-31  20220413194  3602   \n",
       "17   Herr       Mischa Schiwow     AL   1961-08-31  20220413194  4240   \n",
       "\n",
       "                                      Id                  nampi  \n",
       "0   4a1be35e-3f69-48a4-85b1-3fee07b006ae   David Garcia NuñezAL  \n",
       "1   f68282d3-d683-48cf-ade1-c08da3dd76da      Michael SchmidFDP  \n",
       "2   70c4fb44-48da-47b2-b24e-aaf500c1e78c          Heidi EggerSP  \n",
       "3   ce8041b1-19af-4e73-8c30-0b2577592bbd        Stephan ItenSVP  \n",
       "4   510f49c8-76b0-4c37-a031-518867986a26       Thomas KlegerFDP  \n",
       "..                                   ...                    ...  \n",
       "13  32e89ef8-f5b2-46e5-aafe-7dd15929f00a      Markus BaumannGLP  \n",
       "14  3c84712a-df95-48f7-b76e-d95540209bdb  Alan David SanginesSP  \n",
       "15  4a1be35e-3f69-48a4-85b1-3fee07b006ae   David Garcia NuñezAL  \n",
       "16  ddcd41f6-9873-4002-80d8-dfd87114c9a5       Mischa SchiwowAL  \n",
       "17  ddcd41f6-9873-4002-80d8-dfd87114c9a5       Mischa SchiwowAL  \n",
       "\n",
       "[9185 rows x 8 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a4401c4c-8d8f-45a2-b1b6-1ec8f15b040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = uber_df.groupby('Anrede')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cc43f9eb-6265-43ce-a114-c09d41512042",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped['Länge'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "84a9a7e9-4203-4c9e-838c-3251b25440f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7d45c3df-be31-45c9-8043-869f8722cb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anrede\n",
       "Herr    10569041.0\n",
       "Frau     3534972.0\n",
       "Name: Länge, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4024c3-214f-4ab5-8627-8765ce5e54f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
